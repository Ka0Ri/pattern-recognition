{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dang Thanh Vu - 197796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy.stats import norm\n",
    "def TrainDOHMMBaumWelch_Single_Seq_variables(pi_init, A, B, O):\n",
    "    M, N = B.shape[:]\n",
    "    T = len(O)\n",
    "    \n",
    "    #alpha\n",
    "    alpha = np.zeros((T, N))\n",
    "    alpha[0] = B[O[0]].T*pi_init\n",
    "    c = np.zeros(T)\n",
    "    c[0] = 1./np.sum(alpha[0])\n",
    "    alpha[0] = c[0]*alpha[0]\n",
    "    \n",
    "    for t in range(T - 1):\n",
    "        for j in range(N):\n",
    "            alpha[t + 1, j] = B[O[t + 1], j]*np.sum(alpha[t]*A[:, j].T)\n",
    "        c[t+1] = 1./np.sum(alpha[t+1])\n",
    "        alpha[t+1] = c[t+1]*alpha[t+1]\n",
    "    P = -np.sum(np.log10(c))\n",
    "    \n",
    "    beta = np.zeros((T, N))\n",
    "    beta[-1, :] = 1\n",
    "    beta[-1] = beta[-1]*c[-1]\n",
    "    for t in np.arange(T-2, -1, -1):\n",
    "        for i in range(N):\n",
    "            beta[t, i] = np.sum(A[i]*B[O[t + 1]]*beta[t+1])\n",
    "        beta[t] = c[t]*beta[t]\n",
    "    \n",
    "    #ksi\n",
    "    ksi = np.zeros((T - 1, N, N))\n",
    "    for t in range(T-1):\n",
    "        denum = 0\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                denum = denum + alpha[t, i]*A[i, j]*B[O[t+1], j]*beta[t+1, j]\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                ksi[t, i, j] = (alpha[t, i]*A[i, j]*B[O[t+1], j]*beta[t+1, j])/denum\n",
    "    #gamma\n",
    "    gamma = np.zeros((T - 1, N))\n",
    "    for t in range(T - 1):\n",
    "        for i in range(N):\n",
    "            gamma[t, i] = 0\n",
    "            for j in range(N):\n",
    "                gamma[t, i] = gamma[t, i] + ksi[t, i, j]\n",
    "    return alpha, beta, c, P, ksi, gamma\n",
    "\n",
    "def TrainDOHMMBaumWelch_Single_Seq_reest_variables(M,N,T,O,gamma,ksi):\n",
    "    Adenum = np.zeros(N)\n",
    "    Anum = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            Adenum[i] = 0\n",
    "            Anum[i, j] = 0\n",
    "            for t in range(T-1):\n",
    "                Adenum[i] = Adenum[i] + gamma[t, i]\n",
    "                Anum[i, j] = Anum[i, j] + ksi[t, i, j]\n",
    "    \n",
    "    Bdenum = np.zeros(N)\n",
    "    Bnum = np.zeros((M, N))\n",
    "    for j in range(N):\n",
    "        for k in range(M):\n",
    "            Bdenum[j] = 0\n",
    "            Bnum[k,j] = 0\n",
    "            for t in range(T-1):\n",
    "                Bdenum[j] = Bdenum[j] + gamma[t, j]\n",
    "                if(O[t] == k):\n",
    "                    Bnum[k, j] = Bnum[k, j] + gamma[t, j]\n",
    "    return Anum, Adenum, Bnum, Bdenum\n",
    "            \n",
    "def MultSeqTrainDoHMMBWsc(pi_init, A, B, NumericData, maxEpoch):\n",
    "    #M: state of data, N: state, K: number of data, T: data length\n",
    "    M, N = B.shape[:]\n",
    "    K = len(NumericData)\n",
    "    epoch  = 1\n",
    "    curProb = -10000\n",
    "    AllProb = []\n",
    "    \n",
    "    alpha = []; beta = []; c = []; P = []; ksi = []; gamma = []; Anum = []; Adenum = []; Bnum = []; Bdenum = []\n",
    "    \n",
    "    while epoch <= maxEpoch:\n",
    "        for k in range(K):\n",
    "            O = NumericData[k]\n",
    "            T = len(O)\n",
    "            alpha_, beta_, c_, P_, ksi_, gamma_ = TrainDOHMMBaumWelch_Single_Seq_variables(pi_init, A, B, O)\n",
    "            Anum_, Adenum_, Bnum_, Bdenum_ =TrainDOHMMBaumWelch_Single_Seq_reest_variables(M,N,T,O,gamma_,ksi_)\n",
    "            alpha.append(alpha_); beta.append(beta_); P.append(P_); ksi.append(ksi_); gamma.append(gamma_)\n",
    "            Anum.append(Anum_); Adenum.append(Adenum_); Bnum.append(Bnum_); Bdenum.append(Bdenum_)\n",
    "        \n",
    "        pi_init_hat = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            pi_init_hat[i] = 0\n",
    "            for k in range(K):\n",
    "                pi_init_hat[i] = pi_init_hat[i] + gamma[k][0, i]\n",
    "        pi_init_hat = pi_init_hat/np.sum(pi_init_hat)\n",
    "        \n",
    "        A_hat_num = np.zeros((N, N))\n",
    "        A_hat_denum = np.zeros((N, N))\n",
    "        A_hat = np.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                A_hat_num[i, j] = 0\n",
    "                A_hat_denum[i, j] = 0\n",
    "                for k in range(K):\n",
    "                    A_hat_num[i, j] = A_hat_num[i, j] + (1./P[k])*Anum[k][i, j]\n",
    "                    A_hat_denum[i, j] = A_hat_denum[i, j] + (1./P[k])*Adenum[k][i]\n",
    "                A_hat[i, j] = A_hat_num[i, j]/A_hat_denum[i, j]\n",
    "    \n",
    "        \n",
    "        B_hat_num = np.zeros((M, N))\n",
    "        B_hat_denum = np.zeros((M, N))\n",
    "        B_hat = np.zeros((N, N))\n",
    "        for i in range(M):\n",
    "            for j in range(N):\n",
    "                B_hat_num[i, j] = 0\n",
    "                B_hat_denum[i, j] = 0\n",
    "                for k in range(K):\n",
    "                    B_hat_num[i, j] = B_hat_num[i, j] + (1/P[k])*Bnum[k][i, j]\n",
    "                    B_hat_denum[i, j] = B_hat_denum[i, j] + (1/P[k])*Bdenum[k][j]\n",
    "                B_hat[i, j] = B_hat_num[i, j]/B_hat_denum[i, j]\n",
    "            \n",
    "        sumP = np.sum(P)\n",
    "        if(sumP <= curProb):\n",
    "            break\n",
    "        else:\n",
    "            AllProb.append(sumP)\n",
    "            curProb = sumP\n",
    "        \n",
    "        pi_init=pi_init_hat\n",
    "        A=A_hat\n",
    "        B=B_hat\n",
    "        epoch=epoch+1\n",
    "        \n",
    "        \n",
    "\n",
    "    return pi_init, A, B\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BWDoHMMst(pi_init,A,B,O):#Forward\n",
    "    T = len(O)\n",
    "    M, N = B.shape[:]\n",
    "    \n",
    "    alpha = np.zeros((N, T))\n",
    "    alpha[:, 0] = pi_init*B[O[0]].T\n",
    "    for t in range(1, T):\n",
    "        for i in range(N):\n",
    "            alpha[i, t] = np.sum((alpha[:, t-1]*A[:,i]))*(B[O[t], i])\n",
    "    \n",
    "    return np.sum(alpha[:, -1])\n",
    "\n",
    "def VitDoHMMst(pi_init,A,B,O):\n",
    "    T = len(O)\n",
    "    M, N = B.shape[:]\n",
    "    \n",
    "    alpha = np.zeros((N, T))\n",
    "    alpha[:, 0] = pi_init*B[O[0]].T\n",
    "    Pred = np.zeros((N, T))\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        for i in range(N):\n",
    "            temp = alpha[:, t-1]*A[:, i]*B[O[t], i]\n",
    "            alpha[i, t] = np.max(temp)\n",
    "            temp = alpha[:, t-1]*A[:, i]\n",
    "            Pred[i, t] = np.argmax(temp)\n",
    "    \n",
    "    path = np.zeros(T)\n",
    "    p = np.max(alpha[:, -1])\n",
    "    path[-1] = np.argmax(Pred[:, -1])\n",
    "    for t in np.arange(T - 2, -1, -1):\n",
    "        path[t] = Pred[int(path[t + 1]), t + 1]\n",
    "    return p, path\n",
    "\n",
    "def VitCoHMMst(pi_init,A,B,O):\n",
    "    T = len(O)\n",
    "    N, N = A.shape[:]\n",
    "    \n",
    "    alpha = np.zeros((N, T))\n",
    "    for i in range(N):\n",
    "        alpha[i, 0] = pi_init[i]*norm(B[0,i], B[1, i]).pdf(O[0])\n",
    "    Pred = np.zeros((N, T))\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        for i in range(N):\n",
    "            temp = alpha[:, t-1]*A[:, i]*norm(B[0,i], B[1, i]).pdf(O[t])\n",
    "            alpha[i, t] = np.max(temp)\n",
    "            temp = alpha[:, t-1]*A[:, i]\n",
    "            Pred[i, t] = np.argmax(temp)\n",
    "    \n",
    "    path = np.zeros(T)\n",
    "    p = np.max(alpha[:, -1])\n",
    "    path[-1] = np.argmax(Pred[:, -1])\n",
    "    for t in np.arange(T - 2, -1, -1):\n",
    "        path[t] = Pred[int(path[t + 1]), t + 1]\n",
    "    return p, path\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultSeqTrainDoHMMVIT(pi_init, A, B, NumericData, maxEpoch):\n",
    "    L = len(NumericData)\n",
    "    M, N = B.shape[:]\n",
    "    print(pi_init)\n",
    "    epoch=0\n",
    "    AllProb=[]\n",
    "    MatchingProb = np.zeros(L)\n",
    "    while epoch <= maxEpoch:\n",
    "        Atemp = np.zeros((N,N))\n",
    "        Btemp = np.zeros((M, N))\n",
    "        pitemp = np.zeros(N)\n",
    "        \n",
    "        for i in range(L):\n",
    "            O = NumericData[i]\n",
    "            p, Path = VitDoHMMst(pi_init, A, B, O)\n",
    "            MatchingProb[i] = p\n",
    "            Path = Path.astype(int)\n",
    "            for k in range(len(Path) - 1):\n",
    "                Atemp[Path[k],Path[k+1]]=Atemp[Path[k],Path[k+1]]+1#count\n",
    "            for k in range(len(Path)):\n",
    "                Btemp[O[k], Path[k]] = Btemp[O[k], Path[k]] + 1#count\n",
    "            pitemp[Path[0]] = pitemp[Path[0]] + 1#count\n",
    "        AllProb.append(np.sum(MatchingProb))\n",
    "        if (epoch > 1) and (AllProb[epoch] <= AllProb[epoch - 1]):\n",
    "            break\n",
    "        pi_init = pitemp/np.sum(pitemp)\n",
    "        N, N = A.shape[:]\n",
    "        for kkk in range(N):\n",
    "            Atemp[kkk] = Atemp[kkk]/np.sum(Atemp[kkk])\n",
    "            Btemp[:, kkk] = Btemp[:, kkk]/np.sum(Btemp[:, kkk])\n",
    "        A = Atemp\n",
    "        B = Btemp\n",
    "        epoch = epoch + 1\n",
    "    return pi_init, A, B\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"DOHMMTrainingData.mat\")\n",
    "TrainingData = data[\"TrainingData\"][0]\n",
    "Train = []\n",
    "for data in TrainingData:\n",
    "    s = data[0]\n",
    "    num_data = []\n",
    "    for c in s:\n",
    "        if c == 'H':\n",
    "            num_data.append(0)\n",
    "        else:\n",
    "            num_data.append(1)\n",
    "    Train.append(num_data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi  [0.93384223 0.06615777]\n",
      "A  [[0.96002916 0.03997084]\n",
      " [0.30940782 0.69059218]]\n",
      "B  [[0.69097144 0.06529574]\n",
      " [0.30902856 0.93470426]]\n",
      "small change in inital value -> large change in the result\n"
     ]
    }
   ],
   "source": [
    "pi_init = np.array([0.5, 0.5]).T\n",
    "A_init = np.array([[0.6, 0.4], [0.01, 0.99]])\n",
    "B_init = np.array([[0.6, 0.01], [0.4, 0.99]])\n",
    "pi, A, B = MultSeqTrainDoHMMBWsc(pi_init, A_init, B_init, Train[:70], 1000)\n",
    "print(\"pi \", pi)\n",
    "print(\"A \", A)\n",
    "print(\"B \", B)\n",
    "print(\"small change in inital value -> large change in the result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample  70  th with Viterbi score  1.2344324713269179e-13\n",
      "[1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  71  th with Viterbi score  1.0998207096553392e-15\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  72  th with Viterbi score  2.332690921222666e-15\n",
      "[1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  73  th with Viterbi score  1.134831876228945e-14\n",
      "[1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  74  th with Viterbi score  1.0998207096553398e-15\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  75  th with Viterbi score  2.1998796653528368e-16\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  76  th with Viterbi score  4.1739743836639667e-17\n",
      "[1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  77  th with Viterbi score  4.918814099663361e-16\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  78  th with Viterbi score  2.459140697082979e-15\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  79  th with Viterbi score  4.918814099663359e-16\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  80  th with Viterbi score  2.7489562557867612e-14\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  81  th with Viterbi score  4.400235874399468e-17\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  82  th with Viterbi score  1.307039839683028e-17\n",
      "[1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  83  th with Viterbi score  4.400235874399467e-17\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1.]\n",
      "sample  84  th with Viterbi score  2.45914069708298e-15\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1.]\n",
      "sample  85  th with Viterbi score  1.2294371865900854e-14\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1.]\n",
      "sample  86  th with Viterbi score  6.146520195300318e-14\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  87  th with Viterbi score  9.838693725717577e-17\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  88  th with Viterbi score  1.0998207096553394e-15\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  89  th with Viterbi score  6.146520195300318e-14\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  90  th with Viterbi score  2.4591406970829796e-15\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "sample  91  th with Viterbi score  8.155505148278617e-15\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "sample  92  th with Viterbi score  1.6312776963438766e-15\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "sample  93  th with Viterbi score  1.683207010506729e-14\n",
      "[1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "sample  94  th with Viterbi score  1.5057507199101684e-15\n",
      "[1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "sample  95  th with Viterbi score  4.077310955251246e-14\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "sample  96  th with Viterbi score  1.3054462489813035e-17\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "sample  97  th with Viterbi score  2.2489069124902324e-18\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "sample  98  th with Viterbi score  3.262908764334064e-16\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "sample  99  th with Viterbi score  1.4592961830070603e-16\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(70, 100):\n",
    "    p, path = VitDoHMMst(pi_init,A,B,Train[i])\n",
    "    print(\"sample \",i, \" th with Viterbi score \", p)\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
